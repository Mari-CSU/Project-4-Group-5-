OKR 1
------------------------------------------------------------------------------------------------------
Companon is designed to ensure the safety of users above all else.
It is important that the system is able to recognize when a user is in a high-risk mental state and to provide the appropriate resources. 
Key stakeholders consist of primarily young adults and, as they are the most at-risk group for adverse mental health.
Secondary stakeholders also include, crisis hotlines, therapists, and mental-health advocates.
Many young adults lack the financial resources for adequate mental health services, so Companon is designed to help link these two groups together. 
------------------------------------------------------------------------------------------------------

Metric(s) with Experimentation
------------------------------------------------------------------------------------------------------
The main goal of OKR 1 is to provide a user with adequate care based on the responses they share with the chat bot.
A question then arises about how the AI will be able to determine a high-risk user.
To do this, it is important to understand how an AI (really an LLM in this scenario) determines “good” and “bad” input. 

In the article How Large Language Models Work (IBM, 2023), within Machine Learning, it highlights how a model is able to distinguish between two genres of music.
It highlights how a model is able to distinguish between two genres of music.
The initial sample size starts with 20 songs, each labeled with a genre, tempo, and energy.
By determining what tempo and energy generally correlate with a certain genre, the LLMs are able to predict what genre a song belongs to.
This same methodology can be applied to Companon.
By taking transcripts of real-world mental health interactions, the AI can build an understanding of how to correctly respond to our constituents.
To help the AI determine good/bad results, I propose this training and testing process:

  1. The AI must be presented with real-world transcripts to base its training on. 
     In this first phase, we would manually determine if the chatbot’s response is appropriate and continue with iterations we deem successful. 

  2. Once the AI consistently produces “good” results, focus testing can begin. A group of young adults will be brought in to “break” the program. The goal is to          have users extreme and outlandish inputs to really stress test the system. Once the system has been thoroughly tested, it is then ready for public release.

In addition, it is important to consult with actual mental health professionals when determining the quality of results. 
------------------------------------------------------------------------------------------------------

Ethical Impact(s)/Issues
------------------------------------------------------------------------------------------------------
There is an inherent risk when trying to design an AI companion, especially one that is reliant on trying to improve mental health. There are numerous real world examples where people have become overly attached to AI, often using them as supplements for real world interaction. 
IBM. 2023. The ELIZA effect: Why people anthropomorphize AI. IBM. Retrieved October 13, 2025 from https://www.ibm.com/think/articles/eliza-effect. The ELIZA effect is based on a phenomenon where computer scientist, Joseph Weizenbaum, observed that people became overly attached to a chatbot program named ELIZA. One of the program scripts was designed to emulate a psychotherapist, where the chatbot would respond either generically or rephrase whatever the user typed. While simple, people did end up forming an attachment with the program, often getting deeply personal with the chatbot. 

     

